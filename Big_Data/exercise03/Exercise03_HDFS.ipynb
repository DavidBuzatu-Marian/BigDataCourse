{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Big Data &ndash; Exercises</center>\n",
    "## <center>Fall 2022 &ndash; Week 3 &ndash; ETH Zurich</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This week we will cover mostly theoretical aspects of Hadoop and HDFS and we will discuss advantages and limitations of different storage models.\n",
    "\n",
    "#### What is Hadoop?\n",
    "Hadoop provides a **distributed file system** and a\n",
    "**framework for the analysis and transformation** of very **large**\n",
    "data sets using the MapReduce paradigm.\n",
    "\n",
    "Several components are part of this framework. In this course you will study HDFS, MapReduce and HBase while this exercise focuses on HDFS and storage models.\n",
    "\n",
    "\n",
    "| *Component*                |*Description*  |*First developer*  |\n",
    "|----------------------------------------------|---|---|\n",
    "| **HDFS**                  |Distributed file system  |Yahoo!  |\n",
    "| **MapReduce**   |Distributed computation framework   |Yahoo!  |\n",
    "| **HBase**           | Column-oriented table service  |Powerset (Microsoft)  |\n",
    "| Pig  | Dataflow language and parallel execution framework  |Yahoo!   |\n",
    "| Hive            |Data warehouse infrastructure   |Facebook  |\n",
    "| ZooKeeper    |Distributed coordination service   |Yahoo!  |\n",
    "| Chukwa  |System for collecting management data   |Yahoo!  |\n",
    "| Avro                |Data serialization system   |Yahoo! + Cloudera  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Hadoop Distributed File System\n",
    "### 1.1 &ndash; State which of the following statements are true:\n",
    "\n",
    "1. The HDFS namespace is a hierarchy of files and directories.\n",
    "True\n",
    "\n",
    "1. In HDFS, each block of the file is either 64 or 128 megabytes depending on the version and distribution of Hadoop in use, and this *cannot* be changed.\n",
    "False.\n",
    "\n",
    "1. A client wanting to write a file into HDFS, first contacts the NameNode, then sends the data to it. The NameNode will write the data into multiple DataNodes in a pipelined fashion.\n",
    "False.\n",
    "\n",
    "1. A DataNode may execute multiple application tasks for different clients concurrently.\n",
    "True.\n",
    "\n",
    "1. The cluster can have thousands of DataNodes and tens of thousands of HDFS clients per cluster.\n",
    "True.\n",
    "\n",
    "1. HDFS NameNodes keep the namespace in RAM.\n",
    "True.\n",
    "\n",
    "1. The locations of block replicas are part of the persistent checkpoint that the NameNode stores in its native file system.\n",
    "False.\n",
    "\n",
    "1. If the block size is set to 64 megabytes, storing a file of 80 megabytes will actually require 128 megabytes of physical memory (2 blocks of 64 megabytes each).\n",
    "False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### 1.2 &ndash; A typical filesystem block size is 4096 bytes. How large is a block in HDFS? List at least two advantages of such choice.\n",
    "A: A block size in HDFS is either 64MB or 128MB. One advantage to this block size is that files can be broken up into relatively small sized chunks resulting in multiple blocks being replicated across the nodes, offering more reliability and space usage for each file. Another advantage has to do with transfering blocks over the network. With a smaller block size, the required blocks for a file would be much higher, incurring a rather high latency for fetching the required file blocks for operations. On the other hand, having a bigger block size would make it innefficient to send the blocks across network, as they might not be able to serve gigabytes of data for each transfer. Therefore, this sizing allows for better throughput at a good latency cost and also offers reliabilirt as lost packets are easier to be resent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 &ndash; How does the hardware cost grow as function of the amount of data we need to store in a Distributed File System such as HDFS? Why?\n",
    "A: Distributed file systems such as HDFS are built on relatively cheap hardware. In the case of HDFS, the cost of storing data mostly depends on the price of storage systems. Using HDDs would come with the benefit of low costs at an icnreased latency for accessing blocks. However, this is a cost a system such as HDFS will allow due to the added benefits of it, such as consistency and availability. Another thing to point out is that for each stored file, we are going to copy the blocks to at least 2 other locations, meaning that for a file of size X, we actually use at least 3 * x size, without the added storage requirements for metadata and hierarchy incurred on the namenode.\n",
    "\n",
    "Therefore, one can conclude hardware costs grow linearly as a function of amount of data.\n",
    "\n",
    "### 1.4 &ndash; Single Point of Failure\n",
    "\n",
    "1. Which component is the main single point of failure in Hadoop?\n",
    "A: The NameNode is the single point of failure in Hadoop, as it is the only node responsible for storing the file hierarchy, the mappings from files to blocks and the block locations, thought the last piece of information could always be recomputed.\n",
    "\n",
    "1. What is the Secondary NameNode?\n",
    "To solve the issue of single point of failure, a secondary NameNode can be introduced. The NameNode then will write an fsImage containing the file hierarchy and file-to-block mappings into durable storage, while also adding entries to an EditLog for keeping track of what operations have been performed since the last fsImage was built. The secondary NameNode looks at these two files, and a given time period will create a new fsImage using the EditLog information, resulting in an updated fsImage called a checkpoint. These checkpoints help reduce the size of EditLogs and keep the fsImage up to date to the latest changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 &ndash; Scalability, Durability and Performance on HDFS\n",
    "Explain how HDFS accomplishes the following requirements:\n",
    "\n",
    "1. Scalability\n",
    "Scalability is achieved by HDFS using a centralized approach, with enforced independency among nodes that store data. Moreover, it achieves scalability by clearly defining the responsibilities of each node. Therefore, adding new data storage is a matter of creating a new process on a new machine that runs as a DataNode. The system will register this new node as the DataNode will ping the NameNode with updates abouts its health and readiness. On the other hand, turning nodes off has the effect that NameNodes will register the disappearance of them, and will use their replication mechanism such that enough replicas of the files that resided on the turned off nodes are available.\n",
    "\n",
    "1. Durability\n",
    "Due to each file's block being stored in multiple DataNodes, across different racks, HDFS provides durability in case of hardware failure. The idea behind this mechanism is that each block has to be available in at least N locations to enforce its persistency. Moreover, these locations have a specific way in which they are chosen such that hardware failure has the minimum effect on losing the data at any given time. A replica will be stored across racks to minimize rack failure while also improving the speed at which data is delivered.\n",
    "\n",
    "1. High sequential read/write performance\n",
    "To achieve high performance, HDFS relies on the physical locations of the file blocks, the paralellism of reads/writes and block size. Firstly, the locations for storing file blocks are chosen in an efficient manner such that the distance between nodes is minimized. This results in less network delay incurred by sending the packets over long distances. Secondly, due to file blocks residing on different nodes in different racks, clients can be served from multiple locations, resulting in work being distributed over a higher number of nodes, miniziming the amount of work a single node has to do. Finally, the block size allows for fast delivery of packets over the network and small recovery time due to packet loses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File I/O operations and replica management.\n",
    "\n",
    "\n",
    "### 2.1 &ndash; Replication policy\n",
    "Assume your HDFS cluster is made of 3 racks, each containing 3 DataNodes. Assume also the HDFS is configured to use a block size of 100 megabytes and that a client is connecting from outside the datacenter (therefore no DataNode is priviledged). \n",
    "\n",
    "1. The client uploads a file of 150 megabytes. Draw in the picture below a possible blocks configuration according to the default HDFS replica policy. How many replicas are there for each block? Where are these replicas stored?\n",
    "A: The policy is based on the following mechanism:\n",
    "    - A random node is selected for the first replica\n",
    "    - The second replica is written on the closest rack, on a random node.\n",
    "    - The third replica is written on the same rack as the second one, but on a different node.\n",
    "\n",
    "1. Can you find a different policy that, using the same number of replicas, improves the expected availability of a block? Does your solution show any drawbacks?\n",
    "A: One such policy is to store each replica in a sepparate rack. This will increase availability at the cost of increased time required for writing, as more inter-rach communication rounds have to be established for this process. The probability of a rack failure is much smaller than that of a single node, therefore the initial policy seems a better approach.\n",
    "1. Referring to the picture below, assume a block is stored in Node 3, as well as in Node 4 and Node 5. If this block of data has to be processed by a task running on Node 6, which of the three replicas will be actually read by Node 6? \n",
    "A: Due to nodes 4 and 5 residing on the same rack as node 6, the read process will target either of these two. Assuming the distance to them is considered equal as they are in the same rack (incurring the same intra-rack cost), either of nodes 4 or 5 will be selected.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/lRzwDdtmytzyDRR/download\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 &ndash; File read and write data flow.\n",
    "To get an idea of how data flows between the client interacting with HDFS, consider a diagram below which shows main components of HDFS. \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/R7hg8x7YEyTFPvD/download\" width=\"600\">\n",
    "\n",
    "1. Draw the main sequence of events when a client copies a file to HDFS.\n",
    "2. Draw the main sequence of events when a client reads a file from HDFS.\n",
    "3. Why do you think a client writes data directly to datanodes instead of sending it through the namenode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 &ndash; Network topology.\n",
    "HDFS estimates the network bandwidth between two nodes by their distance. The distance from a node to its parent node is assumed to be one. A distance between two nodes can be calculated by summing up thier distances to their closest common ancestor. A shorter distance between two nodes means that the greater bandwidth they can utilize to transfer data. Consider a diagram of a possible hadoop cluster over two datacenters below. \n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/Mk2kI7dkKZNrxul/download\" width=\"700\">\n",
    "\n",
    "Calculate following distances using the distance rule explained above:\n",
    "1. Node 0 and Node 1\n",
    "2. Node 0 and Node 2\n",
    "3. Node 1 and Node 4\n",
    "4. Node 4 and Node 5\n",
    "5. Node 2 and Node 3\n",
    "6. Two processes of Node 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 2\n",
    "2. 4\n",
    "3. 6\n",
    "4. 4\n",
    "5. 2\n",
    "6. 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Storage models\n",
    "### 3.1 &ndash; List two differences between Object Storage and Block Storage.\n",
    "1. Object Storage identifies objects using some form of key, such as bucket Id and user ID. On the other hand, block storage uses a file hierarchy for storing and accessing files.\n",
    "2. Files stored in Object Storage are stored as one big file, whereas Block Storage is based on blocks, where each block file get split into one or more blocks that can then be shared across different machines.\n",
    "\n",
    "### 3.2 &ndash; Compare Object Storage and Block Storage. For each of the following use cases, say which technology better fits the requirements.\n",
    "\n",
    "1. Store Netflix movie files in such a way they are accessible from many client applications at the same time [ *Object storage | Block Storage* ]\n",
    "\n",
    "1. Store experimental and simulation data from CERN [ *Object storage | Block Storage* ]\n",
    "\n",
    "1. Store the auto-backups of iPhone/Android devices [ *Object storage | Block Storage* ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Object Storage. Movies are not big files that would benefit from the block storage approach of Block Storage. On the other hand, for many client applications cocnurrent access, sending a single file over has a bigger benefit than trying to find the blocks every time, across different locations of the blocks and then sending them to each client.\n",
    "2. Block Storage. This data is known to be humongous, therefore Object Storage will not be able to store a single huge file that is over 5TB. Moreover, the data from CERN is mostly going to require appending new information to existing files, which is not possible with Object Storage, but is what Block Storage is based upon.\n",
    "3. Object Storage. Backups should not be that big, and would not require to be read that often. Storing the data in Object Storage will reduce the cost for the storage of the file - as Block Storage would require multiple replicas for each of the block files. Finally, these backups are write-once, and therefore would not benefit from the append functionality of the Block Storage systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working Docker-Hadoop\n",
    "\n",
    "Build and run the Hadoop docker image by `docker-compose up -d` in the `exercise03` directory. If completed successfully, you should be able to browse [`http://localhost:9870/`](http://localhost:9870/) and visualize the web interface of the daemon which should look similar to the following image. In the `Datanodes` tab you should see a single operating datanode.\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/LpWcGWZeU5mipBK/download\" width=\"800\">\n",
    "\n",
    "\n",
    "### Connecting to containers  \n",
    "\n",
    "Each Hadoop cluster is set up in one of the three supported modes:\n",
    "\n",
    "- Local (Standalone) Mode\n",
    "- Pseudo-Distributed Mode\n",
    "- Fully-Distributed Mode\n",
    "\n",
    "By default Hadoop runs in Local Mode but we will run it in the *Pseudo-Distributed Mode*. This will allow you to run Hadoop on a single-node (your computer) simulating a distributed file system, with datanode and namenode running in separate containers. For this excercise you will only need to connect to `namenode` and `datanode` containers. To connect to namenode container can use the Docker dashboard interface by navigating to `docker-hadoop` app, and selecting `CLI` option from the `namenode` container (see image below).\n",
    "\n",
    "<img src=\"https://polybox.ethz.ch/index.php/s/Hdlyhagx3JWbLBy/download\" width=\"700\">\n",
    "\n",
    "Alternatively, you can run `docker exec -it namenode /bin/bash` in a terminal. To connect to a datanode, you can similarly find it in the dashboard or run `docker exec -it namenode /bin/bash` in the terminal. Both approaches will give you shell access on the corresponding container. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 &ndash; Upload a file into HDFS\n",
    "\n",
    "Connect to the namenode by `docker exec -it namenode /bin/bash`.\n",
    "\n",
    "Pick an image file in your computer (or you can also download a random one) and try to upload it to HDFS. You may need to create an empty directory before uploading. (Check [here](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html) for help.)\n",
    "\n",
    "1. Which command do you use to upload from the local file system to HDFS?\n",
    "\n",
    "1. Which information can you find if you use `Utilities -> Browse the file system` in the daemon web interface?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 &ndash; Local File System\n",
    "\n",
    "1. ```bash\n",
    "   docker cp docker-compose.yml namenode:docker-compose.yml \n",
    "   ```\n",
    "Then, use HDFS commands to create a directory, copy the `docker-compose.yml` file from your local file system to HDFS. Use `cat` to check if the file is the same on the local and distributed systems. \n",
    "\n",
    "   *Hint:* you may use the following HDFS commands `-mkdir` for directory, `-copyFromLocal` for uploading the file, and `-cat` for printing them. You may have to first use `docker cp` to copy to file into the namenode container.\n",
    "\n",
    "2. Try to locate the file on a datanode. To connect to a datanode by running:\n",
    "\n",
    "   ```bash\n",
    "   docker exec -it datanode /bin/bash\n",
    "   ```\n",
    "\n",
    "   This will give you shell access to the data node machine. cd into `/hadoop/dfs/data/current/` directory and follow the directories until there are only files. Can you check if the file contents are the same as the one you uploaded? Use `ls -l` to check the size of the file size on the local \n",
    "\n",
    "3. Now try to upload a file to HDFS that is ~150MB. On Unix-based system you can also generate such a file filled with zero using:\n",
    "\n",
    "   ```bash\n",
    "   dd if=/dev/zero of=zeros.dat bs=1M count=150\n",
    "   ```\n",
    "\n",
    "   How many blocks the file is split into?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Demystifying FsImage & Edits, & Checkpoint\n",
    "\n",
    "When the NameNode starts up, or a checkpoint is triggered by a configurable threshold:\n",
    "\n",
    "- It reads the FsImage and EditLog from disk.\n",
    "- It applies all the transactions from the EditLog to the in-memory representation of the FsImage.\n",
    "- It flushes out this new version into a new FsImage on disk.\n",
    "- It truncates the old EditLog because its transactions have been applied to the persistent FsImage.\n",
    "\n",
    "A checkpoint can be triggered:\n",
    "\n",
    "> at a given time interval (dfs.namenode.checkpoint.period) expressed in seconds,\n",
    "> or after a given number of filesystem transactions have accumulated (dfs.namenode.checkpoint.txns).\n",
    "\n",
    "If both of these properties are set, the first threshold to be reached triggers a checkpoint.\n",
    "\n",
    "1. Query the configuration file\n",
    "\n",
    "   - `hdfs getconf -confKey dfs.namenode.checkpoint.period` - 3600s\n",
    "   - `hdfs getconf -confKey dfs.namenode.checkpoint.txns` - 1000000\n",
    "   - The fsimage & edit logs location `hdfs getconf -confKey dfs.namenode.name.dir`, I get something like `file:///hadoop/dfs/name`\n",
    "   - Find the fsimage and edit logs in the `current` directory. They must be named like `fsimage_0000000000000000000` & `edits_inprogress_0000000000000000001` \n",
    "   - Output edits `hdfs oev -p xml -i /hadoop/dfs/name/current/edits_inprogress_0000000000000000001 -o edits.xml `\n",
    "   - Output fsimage `hdfs oiv -p XML -i /hadoop/dfs/name/current/fsimage_0000000000000000000 -o fsimage.xml`\n",
    "\n",
    "2. Can you make sense of the outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.4 Changing Block Size (optional)\n",
    "\n",
    "As explained in the tutorials, to change HDFS configurations you edit `etc/hadoop/core-site.xml` and `etc/hadoop/hdfs-site.xml`. In the docker app, you can modify the variables in the `hadoop.env`. For example, in the following line,\n",
    "\n",
    "```bash \n",
    "# hadoop.env \n",
    "CORE_CONF_fs_defaultFS=hdfs://namenode:9000\n",
    "```\n",
    "\n",
    "`CORE_CONF` corresponds to `core-site.xml`. The second part `fs_defaultFS=hdfs://namenode:9000` will be transformed into:\n",
    "\n",
    "```xml\n",
    "<property>\n",
    "    <name>fs.defaultFS</name><\n",
    "    value>hdfs://namenode:9000\n",
    "    </value>\n",
    "</property>\n",
    "```\n",
    "\n",
    "For more details [see here](https://github.com/big-data-europe/docker-hadoop).\n",
    "\n",
    "Try changing the default block size of HDFS to see its affect on read & write performance. You can change the block size by modifying the follwoing line in `hadoop.env`: `HDFS_CONF_dfs_block_size=1048576` The value `1048576` determines the block size in bytes, which in this case is `2^20 bytes` or 1 megabytes.\n",
    "\n",
    "> **_NOTE:_** that for these configuration changes to take effect you must restart the docker app!\n",
    "\n",
    "1. Create a file with size ~150MB and uploade the file to HDFS. Check number of blocks via the Web interface. \n",
    "\n",
    "2. For each of the following block sizes 1048576, 134217728, measure the time to transfer from local to HDFS, from HDFS to local, and removing the file. \n",
    "\n",
    "You can run the following commands \n",
    "```bash\n",
    "time hadoop fs -copyFromLocal zeros.dat /myfolder/zeros.dat\n",
    "time hadoop fs -get /myfolder/zeros.dat zeros.dat\n",
    "time hadoop fs -rm /myfolder/zeros.dat\n",
    "```\n",
    "Can you make sense of the results?\n",
    "\n",
    "> **_NOTE:_** make sure to remove the files before uploading them, so that no caching will distort the measurements"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-showcode": false,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
